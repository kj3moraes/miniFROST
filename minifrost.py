# -*- coding: utf-8 -*-
"""miniFrost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RONbFh5_K4BcELYcZ46ollXu1GMg9WPs

# miniFrost

I am hoping to build a mini GPT model alogn the lines of Karpathy's nanoGPT tutorial [here](https://www.youtube.com/watch?v=kCc8FmEb1nY). Instead of Shakespeare I will attempt to do Robert Frost's poems because they are more evocative to me.
"""
import pandas as pd
import torch
import torch.nn as nn
from torch.nn import functional as F
import tiktoken
import math

"""Read the data from the file and parse it. I am gonna print the first 5 lines."""

poem_collection = pd.read_csv("robert_frost_collection.csv")
print(poem_collection.head())

all_text = ""

# Clean the NaN value
poem_collection = poem_collection.drop(labels=0, axis=0)
for i, poem in enumerate(poem_collection['Content']):
  all_text = "\n".join([all_text, poem])

print("Length of the text: ", len(all_text))

# Extract all the unique characters that are in the text
uniq_chars = sorted(list(set(all_text)))
print(''.join(uniq_chars))

VOCAB_SIZE = len(uniq_chars)

"""### Encoding and Decoding Functions
These functions will be used to encode and decode the string to a list of integers. I will be using OpenAI's tiktoken library that uses sub-words. It will be shorter than encoding every possible character as its ASCII value.
"""

enc = tiktoken.get_encoding("gpt2")

STOI = { ch:i for i,ch in enumerate(uniq_chars)}
ITOS = { i:ch for i,ch in enumerate(uniq_chars)}
encode = lambda s: [STOI[c] for c in s]
decode = lambda l: ''.join([ITOS[i] for i in l])

"""### Build a Pytorch Tensor from the Encoded Text"""

encoded_text = encode(all_text)
data = torch.tensor(encoded_text)
print(data.shape, data.dtype)
print(data[:500])

"""### Training and Testing Split

At this point, we will have to decide on the training-testing split for the model. The tutorial says a 90-10 split should be a good enough one. 

**Remember that we can alter this later on and see how "accurately" it can generate text as per our needs.**

"""

TRAINING_PORTION = 0.9
n = math.ceil(TRAINING_PORTION * len(data))

training_data = data[:n]
testing_data = data[n:]

"""### Context and Target

Now for a transformer, we need to chunk data in batches and feed it in with a context and the target output that the context "implies".
This is how the model learns. It sees all the context for that batch and sees all the targets and accordingly learns to predict. 
"""

BLOCK_SIZE = 8

# An example here shows the context and target in actions 
context = training_data[:BLOCK_SIZE]
target = training_data[1:BLOCK_SIZE + 1]
for i in range(len(context)):
    print("The context is ", context[:i+1], " and the target is ", target[i])

"""### Getting Batches 

Now, what we want is to sample random batches from the text, get their context and their target and then build a stack out of them. Since our batch size is 8, we will have 8 columns.
We will set the no. of rows in the stack to 4. Pytorch will parallelize this process and *that's what makes transformers so good. The power of efficiency.*

**Extracting Batches**
The function `get_batch` will be used to either extract 4 blocks of size 8 and put them onto a stack togther. 2 [4x8] stacks will be returned. One being the context and the other being the target. 
"""

BATCH_SIZE = 4

torch.manual_seed(1337)

# get_batch will extract from either training or testing depending 
# on the value of the split_type ('train' or 'test')
def get_batch(split_type):
  data = training_data if split_type == "train" else testing_data

  # ix essentially says: find $batch_size (4) random offsets and then 
  # extract $block_size (8) length list after and including it.
  ix = torch.randint((len(data) - BLOCK_SIZE), (BATCH_SIZE, ))

  # Assemble the stacks: context (cx), target (tg)
  cx = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])
  tg = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])
  return cx, tg

# Sampling the 
xd, yd = get_batch('train')

print(xd.shape)
print(xd)
print("---")
print(yd.shape)
print(yd)

for batch in range(BATCH_SIZE):
    for time in range(BLOCK_SIZE):
        context = xd[batch, :time+1]
        target = xd[batch, time]
        print(f'when the input is {context.tolist()} the expected out is {target.tolist()}')
    print()

"""## Bigram Language Model

The simplest language model you can find. It literally just does word prediction based on the last word. Read more about n-gram models [here](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9)
"""

class BigramLanguageModel(nn.Module):

  def __init__(self, vocab_size):
      super().__init__()
      # each token reads of the logits for the next token in the lookup table
      self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

  def forward(self, idx, targets=None):
    # idx is the contexts that will be passed into the embedding table 
    # which retunrns a (Batch, Time, Channel) logit sequence
    logits = self.token_embedding_table(idx) # (B, T, C)

    if targets is None:
      loss = None
    else:
      B, T, C = logits.shape
      logits = logits.view(B*T, C)
      targets = targets.view(B*T)
      # We need to model the loss function as the difference (cross_entropy)
      # between the produced output and the target output
      loss = F.cross_entropy(logits, targets)
    return logits, loss

  def generate(self, idx, max_new_tokens):
    for _ in range(max_new_tokens):
      logits, loss = self(idx)
      logits = logits[:, -1, :]
      probs = F.softmax(logits, dim=-1)
      idx_next = torch.multinomial(probs, num_samples=1)
      idx = torch.cat((idx, idx_next), dim=1)
    return idx

m = BigramLanguageModel(VOCAB_SIZE)

logits, loss = m(xd, yd)
print(logits.shape)
print(loss)

"""Now that we have a generation function, we can try generating some data. Of course it will be random data because we haven't trained our model but it will be useful. """

idx = torch.zeros((1,1), dtype = torch.long)
sequence_gen = m.generate(idx, max_new_tokens=100)[0].tolist()
print(decode(sequence_gen))

"""## Optimization 

We can now start training the model and prompting it to minimizing the loss function. We will use the AdamW optimzer from PyTorch. The learning rate ca be set to much higher for 
"""

# create an optimizer
optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)

"""Now in batches we can train the model to reduce the loss. """

batch_size = 32
for steps in range(1000):
    # get the training data
    xb, yb = get_batch('train')

    # evaluate the loss function
    logits, loss = m(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    print(loss.item())

"""Now lets try outputing again"""

idx = torch.zeros((1,1), dtype = torch.long)
sequence_gen = m.generate(idx, max_new_tokens=100)[0].tolist()
print(decode(sequence_gen))
# -*- coding: utf-8 -*-
"""miniFrost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RONbFh5_K4BcELYcZ46ollXu1GMg9WPs

# miniFrost

I am hoping to build a mini GPT model alogn the lines of Karpathy's nanoGPT tutorial [here](https://www.youtube.com/watch?v=kCc8FmEb1nY). Instead of Shakespeare I will attempt to do Robert Frost's poems because they are more evocative to me.
"""
import pandas as pd
import torch
import torch.nn as nn
from torch.nn import functional as F
import tiktoken
import math

"""Read the data from the file and parse it. I am gonna print the first 5 lines."""

poem_collection = pd.read_csv("robert_frost_collection.csv")
print(poem_collection.head())

all_text = ""

# Clean the NaN value
poem_collection = poem_collection.drop(labels=0, axis=0)
for i, poem in enumerate(poem_collection['Content']):
  all_text = "\n".join([all_text, poem])

print("Length of the text: ", len(all_text))

fileout = open("poems.txt", "w")
fileout.write(all_text)
fileout.close()

# Extract all the unique characters that are in the text
uniq_chars = sorted(list(set(all_text)))
print(''.join(uniq_chars))

VOCAB_SIZE = len(uniq_chars)

"""### Encoding and Decoding Functions
These functions will be used to encode and decode the string to a list of integers. I will be using OpenAI's tiktoken library that uses sub-words. It will be shorter than encoding every possible character as its ASCII value.
"""

enc = tiktoken.get_encoding("gpt2")

STOI = { ch:i for i,ch in enumerate(uniq_chars)}
ITOS = { i:ch for i,ch in enumerate(uniq_chars)}
encode = lambda s: [STOI[c] for c in s]
decode = lambda l: ''.join([ITOS[i] for i in l])

"""### Build a Pytorch Tensor from the Encoded Text"""

encoded_text = encode(all_text)
data = torch.tensor(encoded_text)
print(data.shape, data.dtype)
print(data[:500])

"""### Training and Testing Split

At this point, we will have to decide on the training-testing split for the model. The tutorial says a 90-10 split should be a good enough one. 

**Remember that we can alter this later on and see how "accurately" it can generate text as per our needs.**

"""

TRAINING_PORTION = 0.9
n = math.ceil(TRAINING_PORTION * len(data))

training_data = data[:n]
testing_data = data[n:]

"""### Context and Target

Now for a transformer, we need to chunk data in batches and feed it in with a context and the target output that the context "implies".
This is how the model learns. It sees all the context for that batch and sees all the targets and accordingly learns to predict. 
"""

BLOCK_SIZE = 8

# An example here shows the context and target in actions 
context = training_data[:BLOCK_SIZE]
target = training_data[1:BLOCK_SIZE + 1]
for i in range(len(context)):
    print("The context is ", context[:i+1], " and the target is ", target[i])

"""### Getting Batches 

Now, what we want is to sample random batches from the text, get their context and their target and then build a stack out of them. Since our batch size is 8, we will have 8 columns.
We will set the no. of rows in the stack to 4. Pytorch will parallelize this process and *that's what makes transformers so good. The power of efficiency.*

**Extracting Batches**
The function `get_batch` will be used to either extract 4 blocks of size 8 and put them onto a stack togther. 2 [4x8] stacks will be returned. One being the context and the other being the target. 
"""

BATCH_SIZE = 4
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(1337)

# get_batch will extract from either training or testing depending 
# on the value of the split_type ('train' or 'test')
def get_batch(split_type):
  data = training_data if split_type == "train" else testing_data

  # ix essentially says: find $batch_size (4) random offsets and then 
  # extract $block_size (8) length list after and including it.
  ix = torch.randint((len(data) - BLOCK_SIZE), (BATCH_SIZE, ))

  # Assemble the stacks: context (cx), target (tg)
  cx = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])
  tg = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])
  x, y = cx.to(DEVICE), tg.to(DEVICE)
  return x,y

# Sampling the 
xd, yd = get_batch('train')

print(xd.shape)
print(xd)
print("---")
print(yd.shape)
print(yd)

for batch in range(BATCH_SIZE):
    for time in range(BLOCK_SIZE):
        context = xd[batch, :time+1]
        target = xd[batch, time]
        print(f'when the input is {context.tolist()} the expected out is {target.tolist()}')
    print()

EVAL_ITERS = 200

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(EVAL_ITERS)
        for k in range(EVAL_ITERS):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

"""## Bigram Language Model

The simplest language model you can find. It literally just does word prediction based on the last word. Read more about n-gram models [here](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9)
"""

N_EMBED = 32
class BigramLanguageModel(nn.Module):

  def __init__(self):
      super().__init__()
      # each token reads of the logits for the next token in the lookup table
      self.token_embedding_table = nn.Embedding(VOCAB_SIZE, N_EMBED)
      self.pos_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBED)
      self.lm_head = nn.Linear(N_EMBED, VOCAB_SIZE)

  def forward(self, idx, targets=None):
    B, T = idx.shape

    # idx is the contexts that will be passed into the embedding table 
    # which retunrns a (Batch, Time, Channel) logit sequence
    token_embs = self.token_embedding_table(idx) # (B, T, C)
    pos_embs = self.pos_embedding_table(torch.arange(T, device=DEVICE))
    x = token_embs + pos_embs
    logits = self.lm_head(x) # (B, T, VOCAB_SIZE)

    if targets is None:
      loss = None
    else:
      B,T,C = logits.shape
      logits = logits.view(B*T, C)
      targets = targets.view(B*T)
      # We need to model the loss function as the difference (cross_entropy)
      # between the produced output and the target output
      loss = F.cross_entropy(logits, targets)
    return logits, loss

      
  def generate(self, idx, max_new_tokens):
    # idx is (B, T) array of indices in the current context
    for _ in range(max_new_tokens):
        # get the predictions
        logits, loss = self(idx)
        # focus only on the last time step
        logits = logits[:, -1, :] # becomes (B, C)
        # apply softmax to get probabilities
        probs = F.softmax(logits, dim=-1) # (B, C)
        # sample from the distribution
        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
        # append sampled index to the running sequence
        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
    return idx

model = BigramLanguageModel()
m = model.to(DEVICE)
logits, loss = m(xd, yd)
print(logits.shape)
print(loss)

"""Now that we have a generation function, we can try generating some data. Of course it will be random data because we haven't trained our model but it will be useful. """

idx = torch.zeros((BLOCK_SIZE,BATCH_SIZE), dtype = torch.long)
print(idx)
sequence_gen = m.generate(idx, max_new_tokens=100)[0].tolist()
print(decode(sequence_gen))

"""## Optimization 

We can now start training the model and prompting it to minimizing the loss function. We will use the AdamW optimzer from PyTorch. The learning rate ca be set to much higher for 
"""

# create an optimizer
optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)

"""Now in batches we can train the model to reduce the loss. """

MAX_ITERS = 3000
EVAL_INTERVAL = 300
for iter in range(MAX_ITERS):

    # every once in a while evaluate the loss on train and val sets
    if iter % EVAL_INTERVAL == 0:
        losses = estimate_loss()
        print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

    # sample a batch of data
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# generate from the model
context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)
print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))

"""Now lets try outputing again"""

idx = torch.zeros((1,1), dtype = torch.long)
sequence_gen = m.generate(idx, max_new_tokens=100)[0].tolist()
print(decode(sequence_gen))

"""## Self-Attention and Transformer Networks

Now we want the batches to talk to each other. So if I am at batch i, i want to gain context and loss information of the predications of **all the previous batches** because I am trying to improve the predictions for this one.

I **cannot look at all the predictions in the future batches** because I want to predict the future. 
"""

torch.manual_seed(1337)
B,T,C = 4,8,2 # batch = 4, time = 8 and channel = 2
x = torch.randn(B,T,C)
x.shape
print(x)

"""Now lets calculate the mean logits and use its values for each subsequent time column."""

weights = torch.tril(torch.ones(T,T))
print(weights)
weights = weights / weights.sum(1, keepdim=True)
print(weights)

# This is valid matrix multiplication becasue PyTorch will automatically convert
# the weights (T x T) matrix to a (B x T x T) so it can multiply with a 
# (B x T x C) matrix 
xbow_2 = weights @ x
print(xbow_2[0])

tril = torch.tril(torch.ones(T,T))
weights = torch.zeros(T, T)

# This essentially says that token in the future cannot communicate with a token
# in the past. We can't have a future token interacting with the past for the 
# reasons mentioned before. 
weights = weights.masked_fill(tril==0, float('-inf'))
weights = F.softmax(weights, dim=-1)

print(weights)
xbow3 = weights @ x
print(xbow3)

"""## Self-Attention 

"""

# version 4: self-attention!
torch.manual_seed(1337)
B,T,C = 4,8,32 # batch, time, channels
x = torch.randn(B,T,C)

# A single Head performs self-attention
head_size = 16
key = nn.Linear(C, head_size, bias=False)
query = nn.Linear(C, head_size, bias=False)
value = nn.Linear(C, head_size, bias=False)
k = key(x)   # (B, T, 16)
q = query(x) # (B, T, 16)
wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)